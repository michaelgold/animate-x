data:
  data_dir: "path/to/data"
  batch_size: 32
  num_workers: 4

model:
  clip_feature_dim: 512
  pose_dim: 34  # Number of keypoints * 2 (x, y)
  hidden_dim: 256
  ipi_output_dim: 128
  epi_output_dim: 128
  output_dim: 512
  num_pose_anchors: 100
  num_inference_steps: 1000
  beta_start: 0.0001
  beta_end: 0.02
  mamba_dim: 512
  mamba_state_dim: 16
  mamba_conv_dim: 4
  mamba_expand: 2
  clip_path: "openai/clip-vit-base-patch32"
  dwpose_path: "IDEA-Research/DWPose"

training:
  learning_rate: 1e-4
  max_epochs: 100
  perceptual_weight: 0.1
  clip_grad_norm: 1.0
  lr_factor: 0.5
  lr_patience: 5

evaluation:
  fvd_num_samples: 1000
  fvd_sample_length: 16
